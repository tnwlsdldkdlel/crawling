#!/usr/bin/env python3
"""
ê°œì„ ëœ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (Playwright ì§ì ‘ ì‚¬ìš©)

ì‚¬ìš©ë²•:
    python test_crawling_v2.py <ë„¤ì´ë²„_ë¸”ë¡œê·¸_URL>

ì˜ˆì‹œ:
    python test_crawling_v2.py https://blog.naver.com/example-post
"""

import sys
import json
import asyncio
from typing import Optional, List
from dataclasses import dataclass, asdict
from playwright.async_api import async_playwright


def print_colored(text: str, color: str = "white") -> None:
    """ì»¬ëŸ¬ ì¶œë ¥ í—¬í¼ í•¨ìˆ˜."""
    colors = {
        "green": "\033[92m",
        "red": "\033[91m",
        "yellow": "\033[93m",
        "blue": "\033[94m",
        "white": "\033[0m",
        "bold": "\033[1m",
    }
    end = "\033[0m"
    print(f"{colors.get(color, '')}{text}{end}")


@dataclass
class CrawlResult:
    """í¬ë¡¤ë§ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    extracted_sentence: Optional[str]
    source_url: str
    success: bool
    error_message: Optional[str] = None
    keywords_found: Optional[List[str]] = None

    def to_json(self) -> str:
        """JSON ë¬¸ìì—´ë¡œ ë³€í™˜"""
        return json.dumps(asdict(self), ensure_ascii=False, indent=2)


async def crawl_naver_blog(url: str, keywords: List[str]) -> CrawlResult:
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ì—ì„œ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ë¬¸ì¥ ì¶”ì¶œ.

    Args:
        url: ë„¤ì´ë²„ ë¸”ë¡œê·¸ URL
        keywords: ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸

    Returns:
        CrawlResult ê°ì²´
    """
    try:
        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹¤í–‰
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            # í˜ì´ì§€ ë¡œë“œ
            await page.goto(url, wait_until="networkidle", timeout=30000)

            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” iframe ì‚¬ìš© - iframe ë‚´ë¶€ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
            content = ""
            frames = page.frames

            for frame in frames:
                try:
                    # ë©”ì¸ í”„ë ˆì„ ì°¾ê¸°
                    if "mainFrame" in frame.url or "PostView" in frame.url:
                        content = await frame.inner_text("body")
                        break
                except:
                    continue

            # iframeì—ì„œ ëª» ì°¾ìœ¼ë©´ ë©”ì¸ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            if not content:
                content = await page.inner_text("body")

            await browser.close()

            # í‚¤ì›Œë“œ ê²€ìƒ‰
            found_keywords = []
            sentences_with_keywords = []

            # ì½˜í…ì¸ ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ê°„ë‹¨í•œ ë°©ì‹)
            sentences = content.replace("\n", " ").split(". ")

            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue

                # ì´ ë¬¸ì¥ì— í¬í•¨ëœ í‚¤ì›Œë“œ ì°¾ê¸°
                keywords_in_sentence = [kw for kw in keywords if kw in sentence]

                if keywords_in_sentence:
                    found_keywords.extend(keywords_in_sentence)
                    sentences_with_keywords.append({
                        "sentence": sentence,
                        "keywords": keywords_in_sentence
                    })

            # ê²°ê³¼ ìƒì„±
            if sentences_with_keywords:
                # ê°€ì¥ ë§ì€ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ë¬¸ì¥ ì„ íƒ
                best_match = max(
                    sentences_with_keywords,
                    key=lambda x: len(x["keywords"])
                )

                return CrawlResult(
                    extracted_sentence=best_match["sentence"],
                    source_url=url,
                    success=True,
                    keywords_found=list(set(found_keywords))
                )
            else:
                return CrawlResult(
                    extracted_sentence=None,
                    source_url=url,
                    success=False,
                    error_message=f"í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ë¬¸ì¥ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {', '.join(keywords)}",
                    keywords_found=[]
                )

    except Exception as e:
        return CrawlResult(
            extracted_sentence=None,
            source_url=url,
            success=False,
            error_message=f"í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}"
        )


async def test_crawling(url: str, keywords: List[str]) -> None:
    """í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ë©”ì¸ í•¨ìˆ˜"""
    print_colored("="*80, "blue")
    print_colored("ë„¤ì´ë²„ ë¸”ë¡œê·¸ AI ì½˜í…ì¸  ì¶”ì¶œ í…ŒìŠ¤íŠ¸ v2 (Playwright)", "bold")
    print_colored("="*80, "blue")
    print()

    print_colored(f"ëŒ€ìƒ URL: {url}", "white")
    print_colored(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keywords)}", "white")
    print()

    print_colored("ğŸ” ì½˜í…ì¸  ì¶”ì¶œ ì¤‘...", "yellow")
    result = await crawl_naver_blog(url, keywords)

    print()
    print_colored("="*80, "blue")
    print_colored("ğŸ“Š ì¶”ì¶œ ê²°ê³¼ (JSON)", "bold")
    print_colored("="*80, "blue")
    print()

    # JSON ê²°ê³¼ ì¶œë ¥
    print(result.to_json())
    print()

    # ê²°ê³¼ ìš”ì•½
    print_colored("="*80, "blue")
    if result.success:
        print_colored("âœ… ì„±ê³µ!", "green")
        print_colored(f"ì¶”ì¶œëœ ë¬¸ì¥: {result.extracted_sentence}", "green")
        if result.keywords_found:
            print_colored(f"ë°œê²¬ëœ í‚¤ì›Œë“œ: {', '.join(result.keywords_found)}", "green")
    else:
        print_colored("âŒ ì‹¤íŒ¨", "red")
        print_colored(f"ì˜¤ë¥˜: {result.error_message}", "red")
    print_colored("="*80, "blue")


def main() -> int:
    """ë©”ì¸ í•¨ìˆ˜"""
    if len(sys.argv) < 2:
        print_colored("âŒ ì‚¬ìš©ë²• ì˜¤ë¥˜", "red")
        print()
        print("ì‚¬ìš©ë²•:")
        print(f"  python {sys.argv[0]} <ë„¤ì´ë²„_ë¸”ë¡œê·¸_URL>")
        print()
        print("ì˜ˆì‹œ:")
        print(f"  python {sys.argv[0]} https://blog.naver.com/example-post")
        return 1

    url = sys.argv[1]

    # URL ê²€ì¦
    if not url.startswith("http"):
        print_colored("âš ï¸  ê²½ê³ : ìœ íš¨í•œ URLì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤.", "yellow")
        print()

    # ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì •
    keywords = ["yarn", "ì‹¤", "ë°”ëŠ˜", "ì‚¬ìš©ì‹¤"]

    try:
        asyncio.run(test_crawling(url, keywords))
        return 0
    except KeyboardInterrupt:
        print()
        print_colored("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨", "yellow")
        return 130
    except Exception as e:
        print()
        print_colored(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", "red")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
