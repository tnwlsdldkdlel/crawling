#!/usr/bin/env python3
"""
ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸: í¬ë¡¤ë§ + Supabase ì €ì¥

ì‚¬ìš©ë²•:
    python test_full_pipeline.py <ë„¤ì´ë²„_ë¸”ë¡œê·¸_URL>

ì˜ˆì‹œ:
    python test_full_pipeline.py https://blog.naver.com/example-post
"""

import sys
import json
import asyncio
import os
from typing import Optional, List
from dataclasses import dataclass, asdict
from datetime import datetime
from playwright.async_api import async_playwright
from supabase import create_client, Client
from dotenv import load_dotenv


# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


def print_colored(text: str, color: str = "white") -> None:
    """ì»¬ëŸ¬ ì¶œë ¥ í—¬í¼ í•¨ìˆ˜."""
    colors = {
        "green": "\033[92m",
        "red": "\033[91m",
        "yellow": "\033[93m",
        "blue": "\033[94m",
        "white": "\033[0m",
        "bold": "\033[1m",
    }
    end = "\033[0m"
    print(f"{colors.get(color, '')}{text}{end}")


@dataclass
class CrawlResult:
    """í¬ë¡¤ë§ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    extracted_sentence: Optional[str]
    source_url: str
    success: bool
    error_message: Optional[str] = None
    keywords_found: Optional[List[str]] = None

    def to_json(self) -> str:
        """JSON ë¬¸ìì—´ë¡œ ë³€í™˜"""
        return json.dumps(asdict(self), ensure_ascii=False, indent=2)


class SupabaseManager:
    """Supabase ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ì"""

    def __init__(self):
        """í™˜ê²½ ë³€ìˆ˜ì—ì„œ Supabase í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ë¡œë“œ"""
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_KEY')

        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ë° SUPABASE_KEY í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")

        self.client: Client = create_client(supabase_url, supabase_key)

    def save_extraction(self, result: CrawlResult) -> Optional[int]:
        """ì¶”ì¶œ ê²°ê³¼ë¥¼ Supabaseì— ì €ì¥"""
        try:
            # ë°ì´í„° ì¤€ë¹„
            data = {
                "url": result.source_url,
                "extracted_sentence": result.extracted_sentence,
                "keywords": result.keywords_found if result.keywords_found else []
            }

            # Supabaseì— ì‚½ì…
            response = self.client.table("blog_extractions").insert(data).execute()

            if response.data and len(response.data) > 0:
                record_id = response.data[0].get('id')
                print_colored(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë¨ (ID: {record_id})", "green")
                return record_id
            else:
                print_colored("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: ì‘ë‹µ ë°ì´í„° ì—†ìŒ", "red")
                return None

        except Exception as e:
            print_colored(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}", "red")
            return None


async def crawl_naver_blog(url: str, keywords: List[str]) -> CrawlResult:
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ì—ì„œ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ë¬¸ì¥ ì¶”ì¶œ.

    Args:
        url: ë„¤ì´ë²„ ë¸”ë¡œê·¸ URL
        keywords: ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸

    Returns:
        CrawlResult ê°ì²´
    """
    try:
        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹¤í–‰
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            # í˜ì´ì§€ ë¡œë“œ
            await page.goto(url, wait_until="networkidle", timeout=30000)

            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” iframe ì‚¬ìš© - iframe ë‚´ë¶€ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
            content = ""
            frames = page.frames

            for frame in frames:
                try:
                    # ë©”ì¸ í”„ë ˆì„ ì°¾ê¸°
                    if "mainFrame" in frame.url or "PostView" in frame.url:
                        content = await frame.inner_text("body")
                        break
                except:
                    continue

            # iframeì—ì„œ ëª» ì°¾ìœ¼ë©´ ë©”ì¸ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            if not content:
                content = await page.inner_text("body")

            await browser.close()

            # í‚¤ì›Œë“œ ê²€ìƒ‰
            found_keywords = []
            sentences_with_keywords = []

            # ì½˜í…ì¸ ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ê°„ë‹¨í•œ ë°©ì‹)
            sentences = content.replace("\n", " ").split(". ")

            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue

                # ì´ ë¬¸ì¥ì— í¬í•¨ëœ í‚¤ì›Œë“œ ì°¾ê¸°
                keywords_in_sentence = [kw for kw in keywords if kw in sentence]

                if keywords_in_sentence:
                    found_keywords.extend(keywords_in_sentence)
                    sentences_with_keywords.append({
                        "sentence": sentence,
                        "keywords": keywords_in_sentence
                    })

            # ê²°ê³¼ ìƒì„±
            if sentences_with_keywords:
                # ê°€ì¥ ë§ì€ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ë¬¸ì¥ ì„ íƒ
                best_match = max(
                    sentences_with_keywords,
                    key=lambda x: len(x["keywords"])
                )

                return CrawlResult(
                    extracted_sentence=best_match["sentence"],
                    source_url=url,
                    success=True,
                    keywords_found=list(set(found_keywords))
                )
            else:
                return CrawlResult(
                    extracted_sentence=None,
                    source_url=url,
                    success=False,
                    error_message=f"í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ë¬¸ì¥ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {', '.join(keywords)}",
                    keywords_found=[]
                )

    except Exception as e:
        return CrawlResult(
            extracted_sentence=None,
            source_url=url,
            success=False,
            error_message=f"í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}"
        )


async def test_full_pipeline(url: str, keywords: List[str]) -> None:
    """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸: í¬ë¡¤ë§ + DB ì €ì¥"""
    print_colored("="*80, "blue")
    print_colored("ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸", "bold")
    print_colored("í¬ë¡¤ë§ â†’ Supabase ì €ì¥", "bold")
    print_colored("="*80, "blue")
    print()

    print_colored(f"ëŒ€ìƒ URL: {url}", "white")
    print_colored(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keywords)}", "white")
    print()

    # 1ë‹¨ê³„: í¬ë¡¤ë§
    print_colored("ğŸ“ 1ë‹¨ê³„: ì½˜í…ì¸  ì¶”ì¶œ ì¤‘...", "yellow")
    result = await crawl_naver_blog(url, keywords)

    print()
    print_colored("="*80, "blue")
    print_colored("ğŸ“Š ì¶”ì¶œ ê²°ê³¼ (JSON)", "bold")
    print_colored("="*80, "blue")
    print()
    print(result.to_json())
    print()

    # 2ë‹¨ê³„: Supabase ì €ì¥
    print_colored("ğŸ“ 2ë‹¨ê³„: Supabaseì— ì €ì¥ ì¤‘...", "yellow")
    db_manager = SupabaseManager()
    record_id = db_manager.save_extraction(result)

    print()
    print_colored("="*80, "blue")

    if result.success and record_id:
        print_colored("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!", "green")
        print_colored(f"ì¶”ì¶œëœ ë¬¸ì¥: {result.extracted_sentence}", "green")
        if result.keywords_found:
            print_colored(f"ë°œê²¬ëœ í‚¤ì›Œë“œ: {', '.join(result.keywords_found)}", "green")
        print_colored(f"ë°ì´í„°ë² ì´ìŠ¤ ë ˆì½”ë“œ ID: {record_id}", "green")
    elif not result.success:
        print_colored("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨", "red")
        print_colored(f"ì˜¤ë¥˜: {result.error_message}", "red")
    else:
        print_colored("âš ï¸  í¬ë¡¤ë§ ì„±ê³µ, DB ì €ì¥ ì‹¤íŒ¨", "yellow")

    print_colored("="*80, "blue")


def main() -> int:
    """ë©”ì¸ í•¨ìˆ˜"""
    if len(sys.argv) < 2:
        print_colored("âŒ ì‚¬ìš©ë²• ì˜¤ë¥˜", "red")
        print()
        print("ì‚¬ìš©ë²•:")
        print(f"  python {sys.argv[0]} <ë„¤ì´ë²„_ë¸”ë¡œê·¸_URL>")
        print()
        print("ì˜ˆì‹œ:")
        print(f"  python {sys.argv[0]} https://blog.naver.com/example-post")
        return 1

    url = sys.argv[1]

    # URL ê²€ì¦
    if not url.startswith("http"):
        print_colored("âš ï¸  ê²½ê³ : ìœ íš¨í•œ URLì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤.", "yellow")
        print()

    # ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì •
    keywords = ["yarn", "ì‹¤", "ë°”ëŠ˜", "ì‚¬ìš©ì‹¤"]

    try:
        asyncio.run(test_full_pipeline(url, keywords))
        return 0
    except KeyboardInterrupt:
        print()
        print_colored("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨", "yellow")
        return 130
    except Exception as e:
        print()
        print_colored(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", "red")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
