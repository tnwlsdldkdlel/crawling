#!/usr/bin/env python3
"""
ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ v2: í¬ë¡¤ë§ + LLM êµ¬ì¡°í™” + Supabase ì €ì¥

ì‚¬ìš©ë²•:
    python test_full_pipeline_v2.py <ë„¤ì´ë²„_ë¸”ë¡œê·¸_URL>

ì˜ˆì‹œ:
    python test_full_pipeline_v2.py https://blog.naver.com/example-post
"""

import sys
import json
import asyncio
import os
from typing import Optional, Dict, Any
from dataclasses import dataclass, asdict
from playwright.async_api import async_playwright
from supabase import create_client, Client
from dotenv import load_dotenv
import subprocess

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


def print_colored(text: str, color: str = "white") -> None:
    """ì»¬ëŸ¬ ì¶œë ¥ í—¬í¼ í•¨ìˆ˜."""
    colors = {
        "green": "\033[92m",
        "red": "\033[91m",
        "yellow": "\033[93m",
        "blue": "\033[94m",
        "white": "\033[0m",
        "bold": "\033[1m",
    }
    end = "\033[0m"
    print(f"{colors.get(color, '')}{text}{end}")


@dataclass
class CrawlResult:
    """í¬ë¡¤ë§ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    extracted_data: Optional[Dict[str, Any]]
    source_url: str
    success: bool
    error_message: Optional[str] = None

    def to_json(self) -> str:
        """JSON ë¬¸ìì—´ë¡œ ë³€í™˜"""
        return json.dumps(asdict(self), ensure_ascii=False, indent=2)


class SupabaseManager:
    """Supabase ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ì"""

    def __init__(self):
        """í™˜ê²½ ë³€ìˆ˜ì—ì„œ Supabase í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ë¡œë“œ"""
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_KEY')

        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ë° SUPABASE_KEY í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")

        self.client: Client = create_client(supabase_url, supabase_key)

    def save_extraction(self, result: CrawlResult) -> Optional[int]:
        """ì¶”ì¶œ ê²°ê³¼ë¥¼ Supabaseì— ì €ì¥"""
        try:
            # ë°ì´í„° ì¤€ë¹„
            data = {
                "url": result.source_url,
                "extracted_sentence": result.extracted_data  # JSONBë¡œ ì €ì¥
            }

            # Supabaseì— ì‚½ì… (ì¤‘ë³µ URLì€ ì—…ë°ì´íŠ¸)
            response = self.client.table("extractions").upsert(
                data,
                on_conflict="url"
            ).execute()

            if response.data and len(response.data) > 0:
                record_id = response.data[0].get('id')
                print_colored(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë¨ (ID: {record_id})", "green")
                return record_id
            else:
                print_colored("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: ì‘ë‹µ ë°ì´í„° ì—†ìŒ", "red")
                return None

        except Exception as e:
            print_colored(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}", "red")
            return None


def extract_with_llm(content: str, url: str) -> Optional[Dict[str, Any]]:
    """
    Ollamaì˜ Llama 3ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ ì½˜í…ì¸ ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ

    Args:
        content: ë¸”ë¡œê·¸ ë³¸ë¬¸ í…ìŠ¤íŠ¸
        url: ë¸”ë¡œê·¸ URL

    Returns:
        êµ¬ì¡°í™”ëœ JSON ë°ì´í„° (ë„ì•ˆ, ì‹¤, ë°”ëŠ˜ ì •ë³´)
    """
    prompt = f"""ë‹¤ìŒì€ ëœ¨ê°œì§ˆ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì˜ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.

ì¶”ì¶œí•  ì •ë³´:
- pattern: ë„ì•ˆ ì´ë¦„ ë˜ëŠ” ì„¤ëª… (ì—†ìœ¼ë©´ null)
- yarn: ì‹¤ ì •ë³´ (ì œì¡°ì‚¬, ì œí’ˆëª…, ìƒ‰ìƒ ë“±, ì—†ìœ¼ë©´ null)
- needle: ë°”ëŠ˜ ì •ë³´ (í¬ê¸°, ì¢…ë¥˜ ë“±, ì—†ìœ¼ë©´ null)
- project: í”„ë¡œì íŠ¸ ì´ë¦„ ë˜ëŠ” ë§Œë“  ì‘í’ˆ (ì—†ìœ¼ë©´ null)
- date: ì œì‘ ë‚ ì§œ ë˜ëŠ” ê¸°ê°„ (ì—†ìœ¼ë©´ null)
- url: ì›ë³¸ ë¸”ë¡œê·¸ URL

ì‘ë‹µì€ ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ì´ì–´ì•¼ í•˜ë©°, ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.

ë¸”ë¡œê·¸ ë‚´ìš©:
{content[:3000]}

URL: {url}

JSON ì‘ë‹µ:"""

    try:
        # Ollama CLIë¥¼ í†µí•´ Llama 3 ì‹¤í–‰
        result = subprocess.run(
            ['ollama', 'run', 'llama3'],
            input=prompt,
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode == 0:
            response_text = result.stdout.strip()

            # JSON ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ì´ ìˆì„ ìˆ˜ ìˆìŒ)
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            elif "```" in response_text:
                json_start = response_text.find("```") + 3
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()

            # JSON íŒŒì‹±
            extracted_data = json.loads(response_text)
            return extracted_data
        else:
            print_colored(f"âš ï¸  LLM ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}", "yellow")
            return None

    except subprocess.TimeoutExpired:
        print_colored("âš ï¸  LLM ì‘ë‹µ íƒ€ì„ì•„ì›ƒ", "yellow")
        return None
    except json.JSONDecodeError as e:
        print_colored(f"âš ï¸  LLM ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {e}", "yellow")
        print_colored(f"ì‘ë‹µ ë‚´ìš©: {result.stdout[:200]}", "yellow")
        return None
    except Exception as e:
        print_colored(f"âš ï¸  LLM ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", "yellow")
        return None


async def crawl_naver_blog(url: str) -> CrawlResult:
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ì—ì„œ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•˜ê³  LLMìœ¼ë¡œ êµ¬ì¡°í™”

    Args:
        url: ë„¤ì´ë²„ ë¸”ë¡œê·¸ URL

    Returns:
        CrawlResult ê°ì²´
    """
    try:
        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹¤í–‰
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            # í˜ì´ì§€ ë¡œë“œ
            await page.goto(url, wait_until="networkidle", timeout=30000)

            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” iframe ì‚¬ìš© - iframe ë‚´ë¶€ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
            content = ""
            frames = page.frames

            for frame in frames:
                try:
                    # ë©”ì¸ í”„ë ˆì„ ì°¾ê¸°
                    if "mainFrame" in frame.url or "PostView" in frame.url:
                        content = await frame.inner_text("body")
                        break
                except:
                    continue

            # iframeì—ì„œ ëª» ì°¾ìœ¼ë©´ ë©”ì¸ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            if not content:
                content = await page.inner_text("body")

            await browser.close()

            # LLMìœ¼ë¡œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
            print_colored("ğŸ¤– LLMìœ¼ë¡œ ë°ì´í„° êµ¬ì¡°í™” ì¤‘...", "yellow")
            extracted_data = extract_with_llm(content, url)

            if extracted_data:
                return CrawlResult(
                    extracted_data=extracted_data,
                    source_url=url,
                    success=True
                )
            else:
                return CrawlResult(
                    extracted_data=None,
                    source_url=url,
                    success=False,
                    error_message="LLM ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨"
                )

    except Exception as e:
        return CrawlResult(
            extracted_data=None,
            source_url=url,
            success=False,
            error_message=f"í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}"
        )


async def test_full_pipeline(url: str) -> None:
    """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸: í¬ë¡¤ë§ + LLM + DB ì €ì¥"""
    print_colored("="*80, "blue")
    print_colored("ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ v2 í…ŒìŠ¤íŠ¸", "bold")
    print_colored("í¬ë¡¤ë§ â†’ LLM êµ¬ì¡°í™” â†’ Supabase ì €ì¥", "bold")
    print_colored("="*80, "blue")
    print()

    print_colored(f"ëŒ€ìƒ URL: {url}", "white")
    print()

    # 1ë‹¨ê³„: í¬ë¡¤ë§ + LLM êµ¬ì¡°í™”
    print_colored("ğŸ“ 1ë‹¨ê³„: ì½˜í…ì¸  í¬ë¡¤ë§ ë° LLM êµ¬ì¡°í™” ì¤‘...", "yellow")
    result = await crawl_naver_blog(url)

    print()
    print_colored("="*80, "blue")
    print_colored("ğŸ“Š ì¶”ì¶œ ê²°ê³¼ (JSON)", "bold")
    print_colored("="*80, "blue")
    print()
    print(result.to_json())
    print()

    # 2ë‹¨ê³„: Supabase ì €ì¥
    print_colored("ğŸ“ 2ë‹¨ê³„: Supabaseì— ì €ì¥ ì¤‘...", "yellow")
    db_manager = SupabaseManager()
    record_id = db_manager.save_extraction(result)

    print()
    print_colored("="*80, "blue")

    if result.success and record_id:
        print_colored("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!", "green")
        print_colored(f"ì¶”ì¶œëœ ë°ì´í„°:", "green")
        print(json.dumps(result.extracted_data, ensure_ascii=False, indent=2))
        print_colored(f"\në°ì´í„°ë² ì´ìŠ¤ ë ˆì½”ë“œ ID: {record_id}", "green")
    elif not result.success:
        print_colored("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨", "red")
        print_colored(f"ì˜¤ë¥˜: {result.error_message}", "red")
    else:
        print_colored("âš ï¸  í¬ë¡¤ë§ ì„±ê³µ, DB ì €ì¥ ì‹¤íŒ¨", "yellow")

    print_colored("="*80, "blue")


def main() -> int:
    """ë©”ì¸ í•¨ìˆ˜"""
    if len(sys.argv) < 2:
        print_colored("âŒ ì‚¬ìš©ë²• ì˜¤ë¥˜", "red")
        print()
        print("ì‚¬ìš©ë²•:")
        print(f"  python {sys.argv[0]} <ë„¤ì´ë²„_ë¸”ë¡œê·¸_URL>")
        print()
        print("ì˜ˆì‹œ:")
        print(f"  python {sys.argv[0]} https://blog.naver.com/example-post")
        return 1

    url = sys.argv[1]

    # URL ê²€ì¦
    if not url.startswith("http"):
        print_colored("âš ï¸  ê²½ê³ : ìœ íš¨í•œ URLì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤.", "yellow")
        print()

    try:
        asyncio.run(test_full_pipeline(url))
        return 0
    except KeyboardInterrupt:
        print()
        print_colored("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨", "yellow")
        return 130
    except Exception as e:
        print()
        print_colored(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", "red")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
