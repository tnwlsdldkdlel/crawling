#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ë° í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸

ì‚¬ìš©ë²•:
    python naver_search_crawler.py <ê²€ìƒ‰_í‚¤ì›Œë“œ> [ê°œìˆ˜]

ì˜ˆì‹œ:
    python naver_search_crawler.py "ë§ˆë“¤ë Œìì¼“" 10
"""

import sys
import json
import asyncio
import os
import re
from typing import Optional, Dict, Any, List
from dataclasses import dataclass
from playwright.async_api import async_playwright
from supabase import create_client, Client
from dotenv import load_dotenv
import time

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


def print_colored(text: str, color: str = "white") -> None:
    """ì»¬ëŸ¬ ì¶œë ¥ í—¬í¼ í•¨ìˆ˜."""
    colors = {
        "green": "\033[92m",
        "red": "\033[91m",
        "yellow": "\033[93m",
        "blue": "\033[94m",
        "white": "\033[0m",
        "bold": "\033[1m",
    }
    end = "\033[0m"
    print(f"{colors.get(color, '')}{text}{end}")


class SupabaseManager:
    """Supabase ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ì"""

    def __init__(self):
        """í™˜ê²½ ë³€ìˆ˜ì—ì„œ Supabase í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ë¡œë“œ"""
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_KEY')

        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URL ë° SUPABASE_KEY í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")

        self.client: Client = create_client(supabase_url, supabase_key)

    def url_exists(self, url: str) -> bool:
        """URLì´ ì´ë¯¸ ë°ì´í„°ë² ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        try:
            response = self.client.table("extractions").select("id").eq("url", url).execute()
            return len(response.data) > 0
        except Exception as e:
            print_colored(f"âŒ URL í™•ì¸ ì‹¤íŒ¨: {e}", "red")
            return False

    def save_extraction(self, url: str, keyword: str, extracted_data: Dict[str, Any]) -> Optional[int]:
        """ì¶”ì¶œ ê²°ê³¼ë¥¼ Supabaseì— ì €ì¥"""
        try:
            # extracted_sentenceì—ì„œ project ì œê±°
            extracted_sentence = extracted_data.copy()
            if "project" in extracted_sentence:
                del extracted_sentence["project"]

            # ë°ì´í„° ì¤€ë¹„
            data = {
                "url": url,
                "keyword": keyword,
                "extracted_sentence": extracted_sentence
            }

            # Supabaseì— ì‚½ì… (ì¤‘ë³µ URLì€ ì—…ë°ì´íŠ¸)
            response = self.client.table("extractions").upsert(
                data,
                on_conflict="url"
            ).execute()

            if response.data and len(response.data) > 0:
                record_id = response.data[0].get('id')
                return record_id
            else:
                return None

        except Exception as e:
            print_colored(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}", "red")
            return None


def extract_structured_data(content: str) -> Dict[str, Any]:
    """ë¸”ë¡œê·¸ ì½˜í…ì¸ ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ"""
    extracted = {
        "yarn": None,
        "needle": None,
        "project": None
    }

    # ì‹¤ ì •ë³´ ì°¾ê¸°
    yarn_keywords = ["ì‹¤", "yarn", "ì‚¬ìš©ì‹¤"]
    yarn_brands = ["ë¼ë¼ëœ¨ê°œ", "ì†œì†œëœ¨ê°œ", "ë‹ˆíŠ¸ëŸ¬ë¸Œ", "ì•µì½œìŠ¤ ëœ¨ê°œì‹¤", "ë°”ëŠ˜ì´ì•¼ê¸°"]

    # ë¨¼ì € ë¸Œëœë“œëª…ì´ í¬í•¨ëœ ë¬¸ì¥ ì°¾ê¸°
    for brand in yarn_brands:
        brand_pattern = rf"([^.\n]*{brand}[^.\n]*)"
        yarn_match = re.search(brand_pattern, content, re.IGNORECASE)
        if yarn_match:
            yarn_text = yarn_match.group(1).strip()[:200]

            # ë¶ˆí•„ìš”í•œ ì œëª©/íƒœê·¸ ì œê±°
            yarn_text = re.sub(r'\[.*?\]', '', yarn_text)
            yarn_text = re.sub(r'ëœ¨ê°œì¼ê¸°', '', yarn_text)
            yarn_text = re.sub(r'ìˆ˜ë¯¼ë‹˜?\s*', '', yarn_text)
            yarn_text = re.sub(r'ë§ˆë“¤ë Œ\s*ì?ì¼“', '', yarn_text)
            yarn_text = re.sub(r'cardigan|ìì¼“|ì¡°ë¼|ê°€ë””ê±´|ë² ìŠ¤íŠ¸|ìŠ¤ì›¨í„°', '', yarn_text, flags=re.IGNORECASE)

            # ê´„í˜¸ ë‚´ìš©ë§Œ ì¶”ì¶œ ì‹œë„
            paren_match = re.search(r'\(([^)]+)\)', yarn_text)
            if paren_match:
                yarn_text = paren_match.group(1).strip()

            # mm ì •ë³´ ì œê±°
            yarn_text = re.sub(r'\s*\d+\.?\d*\s*mm.*', '', yarn_text).strip()
            yarn_text = yarn_text.strip('/ ').strip()

            if yarn_text:
                extracted["yarn"] = yarn_text
                break

    # ë¸Œëœë“œë¥¼ ëª» ì°¾ìœ¼ë©´ "yarn :" í˜•ì‹ë§Œ ê²€ìƒ‰ (ì¼ë°˜ "ì‹¤" í‚¤ì›Œë“œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
    if not extracted["yarn"]:
        # "yarn :" í˜•ì‹ë§Œ ê²€ìƒ‰
        yarn_match = re.search(r"yarn\s*[:ï¼š]\s*(.+?)(?:\n|$)", content, re.IGNORECASE)
        if yarn_match:
            yarn_text = yarn_match.group(1).strip()
            # mm ì •ë³´ ì œê±°
            yarn_text = re.sub(r'\s*\d+\.?\d*\s*mm.*', '', yarn_text).strip()
            if len(yarn_text) > 1:
                extracted["yarn"] = yarn_text

    # ë°”ëŠ˜ ì •ë³´ ì°¾ê¸°
    needle_patterns = [
        r"needle\s*[:ï¼š]\s*(.+?)(?:\n|$)",  # needle : ë°¤ë¶€ 4mm
        r"ë°”ëŠ˜\s*[:ï¼š]\s*(.+?)(?:\n|ì‚¬ìš©|$)",  # ë°”ëŠ˜: 4mm
        r"([ê°€-í£\s]*[\d.]+\s*mm)",  # ë°¤ë¶€ 4mm, ì¹˜ì•„ì˜¤êµ¬ 5mm ë“±
    ]
    for pattern in needle_patterns:
        needle = re.search(pattern, content, re.IGNORECASE)
        if needle:
            extracted["needle"] = needle.group(1).strip()[:100]
            break

    # í”„ë¡œì íŠ¸/ì‘í’ˆëª… ì°¾ê¸°
    project_patterns = [
        r"([ê°€-í£]+(?:ìì¼“|ì¡°ë¼|ê°€ë””ê±´|ë² ìŠ¤íŠ¸|ìŠ¤ì›¨í„°|cardigan|vest|sweater))",
        r"FO[:\s]*([^\n]+)"
    ]
    for pattern in project_patterns:
        project = re.search(pattern, content, re.IGNORECASE)
        if project:
            extracted["project"] = project.group(1).strip()[:100]
            break

    return extracted


async def search_naver_blogs(keyword: str, max_results: int = 10, max_pages: int = 3) -> List[str]:
    """
    ë„¤ì´ë²„ì—ì„œ ë¸”ë¡œê·¸ ê²€ìƒ‰í•˜ì—¬ URL ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        max_results: ê°€ì ¸ì˜¬ ìµœëŒ€ ê²°ê³¼ ìˆ˜
        max_pages: ê²€ìƒ‰í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜

    Returns:
        ë¸”ë¡œê·¸ URL ë¦¬ìŠ¤íŠ¸
    """
    blog_urls = []

    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            # ì—¬ëŸ¬ í˜ì´ì§€ ìˆœíšŒ
            for page_num in range(1, max_pages + 1):
                # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ (start: 1, 11, 21, 31, ...)
                start = (page_num - 1) * 10 + 1
                search_url = f"https://search.naver.com/search.naver?where=blog&query={keyword}&start={start}"

                print_colored(f"  í˜ì´ì§€ {page_num} ê²€ìƒ‰ ì¤‘... (start={start})", "blue")
                await page.goto(search_url, wait_until="networkidle", timeout=30000)

                # ë¸”ë¡œê·¸ ë§í¬ ì¶”ì¶œ
                # ëª¨ë“  ë§í¬ë¥¼ ê°€ì ¸ì™€ì„œ blog.naver.com ë§í¬ë§Œ í•„í„°ë§
                all_links = await page.query_selector_all("a")

                page_blog_count = 0
                for link in all_links:
                    href = await link.get_attribute("href")
                    if href and "blog.naver.com" in href:
                        # í¬ìŠ¤íŠ¸ IDê°€ ìˆëŠ” ë§í¬ë§Œ (ì‹¤ì œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸)
                        # ì˜ˆ: https://blog.naver.com/username/223844145249
                        if re.search(r'blog\.naver\.com/[^/]+/\d+', href):
                            # ì¤‘ë³µ ì œê±°
                            if href not in blog_urls:
                                blog_urls.append(href)
                                page_blog_count += 1
                            if len(blog_urls) >= max_results:
                                break

                print_colored(f"    â†’ {page_blog_count}ê°œ ë°œê²¬", "blue")

                # ìµœëŒ€ ê²°ê³¼ ìˆ˜ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
                if len(blog_urls) >= max_results:
                    break

                # í˜ì´ì§€ ê°„ ë”œë ˆì´ (ë„¤ì´ë²„ ì„œë²„ ë¶€í•˜ ë°©ì§€)
                await asyncio.sleep(1)

            await browser.close()

    except Exception as e:
        print_colored(f"âŒ ë„¤ì´ë²„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}", "red")

    return blog_urls


async def crawl_blog_content(url: str) -> Optional[str]:
    """ë¸”ë¡œê·¸ ì½˜í…ì¸  í¬ë¡¤ë§"""
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            await page.goto(url, wait_until="networkidle", timeout=30000)

            # iframe ë‚´ë¶€ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
            content = ""
            frames = page.frames

            for frame in frames:
                try:
                    if "mainFrame" in frame.url or "PostView" in frame.url:
                        content = await frame.inner_text("body")
                        break
                except:
                    continue

            if not content:
                content = await page.inner_text("body")

            await browser.close()
            return content

    except Exception as e:
        print_colored(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨ ({url}): {e}", "red")
        return None


async def process_search_and_crawl(keyword: str, max_results: int = 10, max_pages: int = 3):
    """ê²€ìƒ‰ ë° í¬ë¡¤ë§ ì „ì²´ í”„ë¡œì„¸ìŠ¤"""
    print_colored("="*80, "blue")
    print_colored("ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ë° í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸", "bold")
    print_colored("="*80, "blue")
    print()
    print_colored(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {keyword}", "white")
    print_colored(f"ìµœëŒ€ ê²°ê³¼ ìˆ˜: {max_results}", "white")
    print_colored(f"ê²€ìƒ‰ í˜ì´ì§€: {max_pages}í˜ì´ì§€", "white")
    print()

    # 1ë‹¨ê³„: ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰
    print_colored("ğŸ“ 1ë‹¨ê³„: ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ì¤‘...", "yellow")
    blog_urls = await search_naver_blogs(keyword, max_results, max_pages)

    if not blog_urls:
        print_colored("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", "red")
        return

    print_colored(f"âœ“ {len(blog_urls)}ê°œì˜ ë¸”ë¡œê·¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.", "green")
    print()

    # 2ë‹¨ê³„: ê° ë¸”ë¡œê·¸ í¬ë¡¤ë§ ë° ì €ì¥
    print_colored("ğŸ“ 2ë‹¨ê³„: ë¸”ë¡œê·¸ í¬ë¡¤ë§ ë° ë°ì´í„° ì €ì¥ ì¤‘...", "yellow")
    print()

    db_manager = SupabaseManager()
    success_count = 0
    fail_count = 0
    skip_count = 0

    for idx, url in enumerate(blog_urls, 1):
        print_colored(f"[{idx}/{len(blog_urls)}] {url}", "white")

        # ì´ë¯¸ ì €ì¥ëœ URLì¸ì§€ í™•ì¸
        if db_manager.url_exists(url):
            print_colored(f"  âŠ˜ ì´ë¯¸ ì €ì¥ëœ URL (ê±´ë„ˆë›°ê¸°)", "yellow")
            skip_count += 1
            continue

        # í¬ë¡¤ë§
        content = await crawl_blog_content(url)
        if not content:
            print_colored(f"  âœ— í¬ë¡¤ë§ ì‹¤íŒ¨", "red")
            fail_count += 1
            continue

        # ë°ì´í„° ì¶”ì¶œ
        extracted_data = extract_structured_data(content)

        # yarn AND needle ë‘˜ ë‹¤ ìœ íš¨í•œ ê°’ì´ì–´ì•¼ í•¨
        yarn_valid = extracted_data.get("yarn") and len(extracted_data.get("yarn", "")) > 1
        needle_valid = extracted_data.get("needle") and len(extracted_data.get("needle", "")) > 1

        if not yarn_valid or not needle_valid:
            print_colored(f"  âœ— ì‹¤ê³¼ ë°”ëŠ˜ ì •ë³´ ëª¨ë‘ í•„ìš” (yarn: {yarn_valid}, needle: {needle_valid})", "yellow")
            fail_count += 1
            continue

        # ì €ì¥
        record_id = db_manager.save_extraction(url, keyword, extracted_data)

        if record_id:
            yarn_preview = extracted_data.get('yarn', 'N/A')
            if yarn_preview and yarn_preview != 'N/A':
                yarn_preview = yarn_preview[:30] + "..."
            print_colored(f"  âœ“ ì €ì¥ ì™„ë£Œ (ID: {record_id}) - yarn: {yarn_preview}", "green")
            success_count += 1
        else:
            print_colored(f"  âœ— ì €ì¥ ì‹¤íŒ¨", "red")
            fail_count += 1

        # ë„¤ì´ë²„ ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
        await asyncio.sleep(1)

    # ê²°ê³¼ ìš”ì•½
    print()
    print_colored("="*80, "blue")
    print_colored("ì²˜ë¦¬ ì™„ë£Œ!", "bold")
    print_colored("="*80, "blue")
    print_colored(f"âœ“ ì„±ê³µ: {success_count}ê°œ", "green")
    print_colored(f"âœ— ì‹¤íŒ¨: {fail_count}ê°œ", "red")
    print_colored(f"âŠ˜ ê±´ë„ˆë›°ê¸°: {skip_count}ê°œ (ì´ë¯¸ ì €ì¥ë¨)", "yellow")
    print_colored(f"ì´ {len(blog_urls)}ê°œ ì¤‘ {success_count}ê°œ ì €ì¥ë¨", "white")


def main() -> int:
    """ë©”ì¸ í•¨ìˆ˜"""
    if len(sys.argv) < 2:
        print_colored("âŒ ì‚¬ìš©ë²• ì˜¤ë¥˜", "red")
        print()
        print("ì‚¬ìš©ë²•:")
        print(f"  python {sys.argv[0]} <ê²€ìƒ‰_í‚¤ì›Œë“œ> [ê°œìˆ˜]")
        print()
        print("ì˜ˆì‹œ:")
        print(f"  python {sys.argv[0]} ë§ˆë“¤ë Œìì¼“ 10")
        return 1

    keyword = sys.argv[1]
    max_results = int(sys.argv[2]) if len(sys.argv) > 2 else 10

    try:
        asyncio.run(process_search_and_crawl(keyword, max_results))
        return 0
    except KeyboardInterrupt:
        print()
        print_colored("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨", "yellow")
        return 130
    except Exception as e:
        print()
        print_colored(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", "red")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
